#!/usr/bin/env python3
import pandas as pd
import os
import sys
import json
import subprocess
import time
import pickle
import numpy as np
from datetime import datetime
from pathlib import Path
import logging
from typing import List, Dict, Tuple, Optional
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from concurrent.futures import ThreadPoolExecutor, as_completed

class AlphaFoldCompletePipeline:
    """
    Complete pipeline for AlphaFold batch processing with automated analysis
    """
    
    def __init__(self, config_file="alphafold_pipeline_config.json"):
        self.config = self.load_config(config_file)
        self.setup_logging()
        self.small_proteins = {
            'E': {
                'name': 'EGF',
                'sequence': 'NSDSECPLSHDGYCLHDGVCMYIEALDKYACNCVVGYIGERCQYRDLKWWELR'
            },
            'T': {
                'name': 'TGFa',
                'sequence': 'VVSHFNDCPDSHTQFCFHGTCRFLVQEDKPACVCHSGYVGARCEHADLLA'
            },
            'B': {
                'name': 'Betacellulin',
                'sequence': 'DGNSTSPETNGLLCGDPEENCAATCKDLGCCGVCGDNYTCTCPEGYTGVRCEHVFL'
            }
        }
        self.batch_registry = self.load_batch_registry()
        
    def load_config(self, config_file):
        """Load or create configuration"""
        default_config = {
            # Local paths (Windows)
            "local": {
                "excel_file": r"membrane_proteins.xlsx",
                "sheet_name": "Sheet1",
                "output_dir": r"C:\Users\YourName\alphafold_pipeline",
                "results_excel": r"alphafold_results.xlsx"
            },
            # Remote paths (JHPCE)
            "remote": {
                "base_dir": "/fastscratch/myscratch/jpark2/alphafold",
                "data_dir": "/legacy/alphafold/data",
                "email": "your_email@jhu.edu",
                "username": "jpark2"
            },
            # Batch settings
            "batch": {
                "max_proteins_per_batch": 10,
                "max_sequence_length": 2500,
                "gpu_memory_gb": 64,
                "time_limit_hours": 72,
                "cpus_per_task": 8,
                "partition": "gpu"
            },
            # AlphaFold settings
            "alphafold": {
                "version": "2.3.1",
                "model_preset": "multimer",
                "db_preset": "full_dbs",
                "max_template_date": "2020-05-14",
                "num_models": 5  # AlphaFold generates 5 models by default
            },
            # Analysis settings
            "analysis": {
                "iptm_threshold": 0.7,
                "confidence_threshold": 0.5,
                "high_iptm_threshold": 0.75
            }
        }
        
        if os.path.exists(config_file):
            with open(config_file, 'r') as f:
                loaded_config = json.load(f)
                # Deep merge with defaults
                self._deep_merge(default_config, loaded_config)
                return default_config
        else:
            # Save default config
            with open(config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
            print(f"Created default config file: {config_file}")
            print("Please update it with your specific paths and settings.")
            return default_config
    
    def _deep_merge(self, base, update):
        """Deep merge configuration dictionaries"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._deep_merge(base[key], value)
            else:
                base[key] = value
    
    def setup_logging(self):
        """Setup comprehensive logging system"""
        log_dir = os.path.join(self.config["local"]["output_dir"], "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"pipeline_{timestamp}.log")
        
        # Create formatters
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
        )
        console_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # File handler
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(file_formatter)
        
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(console_formatter)
        
        # Setup logger
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.logger.info(f"Logging initialized. Log file: {log_file}")
    
    def load_batch_registry(self):
        """Load or create batch registry"""
        registry_file = os.path.join(self.config["local"]["output_dir"], "batch_registry.json")
        if os.path.exists(registry_file):
            with open(registry_file, 'r') as f:
                return json.load(f)
        return {}
    
    def save_batch_registry(self):
        """Save batch registry with backup"""
        registry_file = os.path.join(self.config["local"]["output_dir"], "batch_registry.json")
        
        # Create backup if file exists
        if os.path.exists(registry_file):
            backup_file = registry_file.replace('.json', f'_backup_{datetime.now():%Y%m%d_%H%M%S}.json')
            os.rename(registry_file, backup_file)
        
        with open(registry_file, 'w') as f:
            json.dump(self.batch_registry, f, indent=2)
    
    # [Previous methods from the earlier implementation...]
    # Including: read_membrane_proteins, optimize_batches, create_batch_files, etc.
    
    def generate_analysis_scripts(self, batch_info: Dict) -> Dict[str, str]:
        """Generate analysis scripts for a batch"""
        batch_name = batch_info['batch_name']
        local_dir = batch_info['local_dir']
        remote_output_dir = batch_info['remote_output_dir']
        
        # IPTM extraction script
        iptm_script = f"""#!/usr/bin/env python3
import os
import pickle
import json
from datetime import datetime

# Configuration
base_dir = "{remote_output_dir}"
results_file = "iptm_results_{batch_name}.json"

results = {{
    "batch_name": "{batch_name}",
    "analysis_date": datetime.now().isoformat(),
    "proteins": {{}}
}}

print(f"Analyzing batch: {batch_name}")
print(f"Output directory: {{base_dir}}")

for output_folder in sorted(os.listdir(base_dir)):
    output_path = os.path.join(base_dir, output_folder)
    if not os.path.isdir(output_path):
        continue
    
    protein_name = output_folder.replace("_output", "")
    results["proteins"][protein_name] = {{
        "models": [],
        "best_iptm": 0,
        "best_ptm": 0,
        "best_confidence": 0,
        "best_plddt": 0,
        "models_above_threshold": 0
    }}
    
    print(f"\\n=== Analyzing {{protein_name}} ===")
    
    # Find the results directory
    result_dir = None
    for item in os.listdir(output_path):
        if os.path.isdir(os.path.join(output_path, item)):
            result_dir = os.path.join(output_path, item)
            break
    
    if not result_dir:
        print(f"  No results directory found for {{protein_name}}")
        continue
    
    # Analyze each model
    model_results = []
    for filename in sorted(os.listdir(result_dir)):
        if filename.startswith("result_model") and filename.endswith(".pkl"):
            model_num = filename.replace("result_model_", "").replace("_multimer_v3_pred_0.pkl", "")
            
            try:
                with open(os.path.join(result_dir, filename), "rb") as f:
                    result = pickle.load(f)
                
                iptm = float(result.get('iptm', 0))
                ptm = float(result.get('ptm', 0))
                plddt = result.get('plddt', [])
                
                if plddt:
                    mean_plddt = float(sum(plddt) / len(plddt))
                    min_plddt = float(min(plddt))
                    max_plddt = float(max(plddt))
                else:
                    mean_plddt = min_plddt = max_plddt = 0
                
                confidence = iptm * ptm
                
                model_result = {{
                    "model": int(model_num),
                    "iptm": round(iptm, 4),
                    "ptm": round(ptm, 4),
                    "confidence": round(confidence, 4),
                    "plddt_mean": round(mean_plddt, 2),
                    "plddt_min": round(min_plddt, 2),
                    "plddt_max": round(max_plddt, 2)
                }}
                
                model_results.append(model_result)
                
                print(f"  Model {{model_num}}: ipTM={{iptm:.4f}}, pTM={{ptm:.4f}}, "
                      f"confidence={{confidence:.4f}}, pLDDT={{min_plddt:.1f}}-{{max_plddt:.1f}} (mean={{mean_plddt:.1f}})")
                
            except Exception as e:
                print(f"  Error processing {{filename}}: {{e}}")
    
    if model_results:
        # Sort by ipTM
        model_results.sort(key=lambda x: x['iptm'], reverse=True)
        
        # Update summary statistics
        best_model = model_results[0]
        results["proteins"][protein_name]["models"] = model_results
        results["proteins"][protein_name]["best_iptm"] = best_model["iptm"]
        results["proteins"][protein_name]["best_ptm"] = best_model["ptm"]
        results["proteins"][protein_name]["best_confidence"] = best_model["confidence"]
        results["proteins"][protein_name]["best_plddt"] = best_model["plddt_mean"]
        results["proteins"][protein_name]["plddt_range"] = f"{{best_model['plddt_min']}}-{{best_model['plddt_max']}}"
        
        # Count models above threshold
        models_above = sum(1 for m in model_results if m['iptm'] > {self.config['analysis']['iptm_threshold']})
        results["proteins"][protein_name]["models_above_threshold"] = models_above
        
        print(f"  Best model: ipTM={{best_model['iptm']}}, {{models_above}}/{len(model_results)} models with ipTM>{self.config['analysis']['iptm_threshold']}")

# Save results
with open(results_file, 'w') as f:
    json.dump(results, f, indent=2)

print(f"\\nResults saved to: {{results_file}}")

# Print summary
total_proteins = len(results["proteins"])
successful_proteins = sum(1 for p in results["proteins"].values() if p["best_iptm"] > {self.config['analysis']['iptm_threshold']})
print(f"\\n=== SUMMARY ===")
print(f"Total proteins analyzed: {{total_proteins}}")
print(f"Proteins with ipTM > {self.config['analysis']['iptm_threshold']}: {{successful_proteins}} ({{successful_proteins/total_proteins*100:.1f}}%)")
"""
        
        # Save scripts
        iptm_path = os.path.join(local_dir, f"analyze_{batch_name}.py")
        with open(iptm_path, 'w', newline='\n') as f:
            f.write(iptm_script)
        os.chmod(iptm_path, 0o755)
        
        # Create download script for results
        download_script = f"""#!/bin/bash
# Download results for {batch_name}

echo "Downloading results for {batch_name}..."

# Download analysis results
scp {self.config['remote']['username']}@jhpce:~/iptm_results_{batch_name}.json ./

# Download PDB files for best models
mkdir -p pdb_files_{batch_name}

# You can add specific PDB download commands here based on the analysis results

echo "Download complete!"
"""
        
        download_path = os.path.join(local_dir, f"download_results_{batch_name}.sh")
        with open(download_path, 'w', newline='\n') as f:
            f.write(download_script)
        os.chmod(download_path, 0o755)
        
        return {
            'iptm_script': iptm_path,
            'download_script': download_path
        }
    
    def create_results_excel(self, all_results: Dict[str, Dict]) -> str:
        """Create comprehensive Excel report with all results"""
        excel_path = os.path.join(
            self.config["local"]["output_dir"], 
            f"alphafold_results_{datetime.now():%Y%m%d_%H%M%S}.xlsx"
        )
        
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Summary sheet
            summary_data = []
            
            for sp_key, sp_results in all_results.items():
                for batch_name, batch_data in sp_results.items():
                    if 'proteins' not in batch_data:
                        continue
                    
                    for protein_name, protein_data in batch_data['proteins'].items():
                        summary_data.append({
                            'Small Protein': self.small_proteins[sp_key]['name'],
                            'Batch': batch_name,
                            'Membrane Protein': protein_name,
                            'Best ipTM': protein_data.get('best_iptm', 0),
                            'Best pTM': protein_data.get('best_ptm', 0),
                            'Best Confidence': protein_data.get('best_confidence', 0),
                            'pLDDT Range': protein_data.get('plddt_range', 'N/A'),
                            'Mean pLDDT': protein_data.get(' best_plddt', 0),
                            f'Models > {self.config["analysis"]["iptm_threshold"]}': 
                                f"{protein_data.get('models_above_threshold', 0)}/{self.config['alphafold']['num_models']}"
                        })
            
            if summary_data:
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                
                # Format the summary sheet
                worksheet = writer.sheets['Summary']
                
                # Header formatting
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                header_alignment = Alignment(horizontal="center", vertical="center")
                
                for col in range(1, len(summary_df.columns) + 1):
                    cell = worksheet.cell(row=1, column=col)
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = header_alignment
                
                # Conditional formatting for high-confidence results
                for row in range(2, len(summary_df) + 2):
                    iptm_cell = worksheet.cell(row=row, column=4)  # Best ipTM column
                    if iptm_cell.value and float(iptm_cell.value) > self.config['analysis']['iptm_threshold']:
                        for col in range(1, len(summary_df.columns) + 1):
                            cell = worksheet.cell(row=row, column=col)
                            cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
                
                # Auto-adjust column widths
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            # Detailed sheets for each small protein
            for sp_key, sp_results in all_results.items():
                sp_name = self.small_proteins[sp_key]['name']
                detailed_data = []
                
                for batch_name, batch_data in sp_results.items():
                    if 'proteins' not in batch_data:
                        continue
                    
                    for protein_name, protein_data in batch_data['proteins'].items():
                        for model in protein_data.get('models', []):
                            detailed_data.append({
                                'Batch': batch_name,
                                'Membrane Protein': protein_name,
                                'Model': model['model'],
                                'ipTM': model['iptm'],
                                'pTM': model['ptm'],
                                'Confidence': model['confidence'],
                                'Mean pLDDT': model['plddt_mean'],
                                'Min pLDDT': model['plddt_min'],
                                'Max pLDDT': model['plddt_max']
                            })
                
                if detailed_data:
                    detailed_df = pd.DataFrame(detailed_data)
                    detailed_df.to_excel(writer, sheet_name=f'{sp_name}_Details', index=False)
        
        self.logger.info(f"Results Excel file created: {excel_path}")
        return excel_path
    
    def run_complete_pipeline(self, small_protein_keys: List[str] = ['E', 'T', 'B']):
        """Run the complete pipeline from start to finish"""
        self.logger.info("Starting complete AlphaFold pipeline")
        
        # Step 1: Read membrane proteins
        membrane_df = self.read_membrane_proteins()
        
        # Step 2: Generate batches for each small protein
        all_batch_info = {}
        
        for sp_key in small_protein_keys:
            self.logger.info(f"\nProcessing {self.small_proteins[sp_key]['name']} batches...")
            
            # Create optimized batches
            batches = self.optimize_batches(membrane_df, sp_key)
            
            sp_batch_info = {}
            for i, batch_df in enumerate(batches, 1):
                # Create all files for this batch
                batch_info = self.create_batch_files(batch_df, sp_key, i)
                
                # Generate analysis scripts
                analysis_scripts = self.generate_analysis_scripts(batch_info)
                batch_info['analysis_scripts'] = analysis_scripts
                
                sp_batch_info[batch_info['batch_name']] = batch_info
            
            all_batch_info[sp_key] = sp_batch_info
        
        # Save batch registry
        self.batch_registry = all_batch_info
        self.save_batch_registry()
        
        # Step 3: Generate master submission script
        self.generate_master_scripts()
        
        self.logger.info("\nPipeline setup complete!")
        self.logger.info(f"Total batches created: {sum(len(sp) for sp in all_batch_info.values())}")
        self.logger.info("\nNext steps:")
        self.logger.info("1. Upload files to JHPCE using the upload scripts in each batch directory")
        self.logger.info("2. Run submit_all_jobs.sh on JHPCE to start all AlphaFold predictions")
        self.logger.info("3. Monitor progress with monitor_jobs.sh")
        self.logger.info("4. Run analyze_all_results.sh to analyze completed batches")
        self.logger.info("5. Download results and run compile_results.py to create Excel report")
    
    def generate_master_scripts(self):
        """Generate master control scripts"""
        scripts_dir = os.path.join(self.config["local"]["output_dir"], "master_scripts")
        os.makedirs(scripts_dir, exist_ok=True)
        
        # 1. Master upload script
        upload_script = """#!/bin/bash
# Master upload script for all batches

echo "Uploading all batches to JHPCE..."

"""
        for sp_key, sp_batches in self.batch_registry.items():
            upload_script += f"# {self.small_proteins[sp_key]['name']} batches\n"
            for batch_name, batch_info in sp_batches.items():
                upload_script += f"echo \"Uploading {batch_name}...\"\n"
                upload_script += f"cd {batch_info['local_dir']}\n"
                upload_script += f"./upload_to_jhpce.sh\n"
                upload_script += "cd -\n\n"
        
        with open(os.path.join(scripts_dir, "upload_all_batches.sh"), 'w', newline='\n') as f:
            f.write(upload_script)
        
        # 2. Master submission script (for JHPCE)
        submit_script = f"""#!/bin/bash
# Master submission script for all AlphaFold jobs

echo "Submitting all AlphaFold jobs..."

"""
        for sp_key, sp_batches in self.batch_registry.items():
            submit_script += f"# {self.small_proteins[sp_key]['name']} batches\n"
            for batch_name, batch_info in sp_batches.items():
                submit_script += f"echo \"Submitting {batch_name}...\"\n"
                submit_script += f"sbatch {batch_info['remote_fasta_dir']}/../{batch_name}.slurm\n"
                submit_script += "sleep 2\n\n"
        
        with open(os.path.join(scripts_dir, "submit_all_jobs.sh"), 'w', newline='\n') as f:
            f.write(submit_script)
        
        # 3. Monitor script
        monitor_script = """#!/bin/bash
# Monitor all running jobs

while true; do
    clear
    echo "=== AlphaFold Job Status ==="
    echo "Time: $(date)"
    echo ""
    
    squeue -u $USER --format="%.18i %.9P %.30j %.8u %.2t %.10M %.6D %R"
    
    echo ""
    echo "Press Ctrl+C to exit"
    sleep 60
done
"""
        
        with open(os.path.join(scripts_dir, "monitor_jobs.sh"), 'w', newline='\n') as f:
            f.write(monitor_script)
        
        # 4. Analysis runner script
        analysis_script = """#!/bin/bash
# Run analysis on completed batches

echo "Running analysis on completed batches..."

"""
        for sp_key, sp_batches in self.batch_registry.items():
            analysis_script += f"# {self.small_proteins[sp_key]['name']} batches\n"
            for batch_name, batch_info in sp_batches.items():
                analysis_script += f"echo \"Analyzing {batch_name}...\"\n"
                analysis_script += f"python3 analyze_{batch_name}.py\n\n"
        
        with open(os.path.join(scripts_dir, "analyze_all_results.sh"), 'w', newline='\n') as f:
            f.write(analysis_script)
        
        # 5. Results compilation script
        compile_script = f"""#!/usr/bin/env python3
# Compile all results into Excel report

import json
import os
from datetime import datetime

# Import the pipeline class
import sys
sys.path.append('{os.path.dirname(os.path.abspath(__file__))}')
from {os.path.basename(__file__).replace('.py', '')} import AlphaFoldCompletePipeline

# Initialize pipeline
pipeline = AlphaFoldCompletePipeline()

# Load all results
all_results = {{}}
results_dir = "."

for sp_key in ['E', 'T', 'B']:
    all_results[sp_key] = {{}}
    
    for filename in os.listdir(results_dir):
        if filename.startswith(f"iptm_results_{{sp_key}}batch") and filename.endswith(".json"):
            with open(filename, 'r') as f:
                batch_results = json.load(f)
                batch_name = batch_results['batch_name']
                all_results[sp_key][batch_name] = batch_results

# Create Excel report
excel_path = pipeline.create_results_excel(all_results)
print(f"Results compiled to: {{excel_path}}")
"""
        
        with open(os.path.join(scripts_dir, "compile_results.py"), 'w') as f:
            f.write(compile_script)
        
        # Make all scripts executable
        for script in os.listdir(scripts_dir):
            os.chmod(os.path.join(scripts_dir, script), 0o755)
        
        self.logger.info(f"Master scripts created in: {scripts_dir}")

# Main execution
if __name__ == '__main__':
    # Initialize pipeline
    pipeline = AlphaFoldCompletePipeline()
    
    # Run complete pipeline
    pipeline.run_complete_pipeline()
